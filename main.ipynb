{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "logistic_params = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_imputed.drop(\"risk\", axis=1)\n",
    "y = data_imputed['risk']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_rf(trial):\n",
    "\n",
    "    params = {\n",
    "        'n_estimators':trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        'max_depth':trial.suggest_int(\"max_depth\", 1, 32, log=True),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False])\n",
    "    }\n",
    "\n",
    "\n",
    "    rf_model = RandomForestClassifier(\n",
    "        **params,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return cross_val_score(rf_model, X_test, y_test, cv=5).mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_rf, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {trial.value}\")\n",
    "print(f\"Best hyperparameters: {trial.params}\")\n",
    "\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_slice(study)\n",
    "optuna.visualization.plot_contour(study, params=[\"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\"])\n",
    "optuna.visualization.plot_param_importances(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = RandomForestClassifier(**trial.params)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "final_preds = best_rf_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_preds)\n",
    "print(final_accuracy)\n",
    "with open(\"rf_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_rf_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),  # L2\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True)    # L1 \n",
    "    }\n",
    "    \n",
    "    xgb_model = XGBClassifier(**params, use_label_encoder=False)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    preds = xgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_xgb, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {trial.value}\")\n",
    "print(f\"Best hyperparameters: {trial.params}\")\n",
    "\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_slice(study)\n",
    "optuna.visualization.plot_contour(study, params=[\"n_estimators\", \"max_depth\", \"learning_rate\", \"subsample\", \"colsample_bytree\", \"lambda\", \"alpha\"])\n",
    "optuna.visualization.plot_param_importances(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_model = XGBClassifier(**trial.params, tree_method='gpu_hist', use_label_encoder=False)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "final_preds = best_xgb_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_preds)\n",
    "print(final_accuracy)\n",
    "with open(\"xgb_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_xgb_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lgr(trial):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-4, 10.0, log=True),  # Regularization strength\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "        'solver': 'liblinear'\n",
    "    }\n",
    "    \n",
    "    lgr_model = LogisticRegression(**params, random_state=42, n_jobs=-1)\n",
    "    lgr_model.fit(X_train, y_train)\n",
    "\n",
    "    preds = lgr_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective_rf, n_trials=100)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {trial.value}\")\n",
    "print(f\"Best hyperparameters: {trial.params}\")\n",
    "\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_slice(study)\n",
    "optuna.visualization.plot_contour(study, params=[\"C\", \"penalty\"])\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgr_model = LogisticRegression(**trial.params, random_state=42)\n",
    "best_lgr_model.fit(X_train, y_train)\n",
    "final_preds = best_lgr_model.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, final_preds)\n",
    "print(final_accuracy)\n",
    "with open(\"lgr_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(best_lgr_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"{model_name} Evaluation:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "    plt.savefig(model_name+'.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lgr_model.pkl\", 'rb') as file:\n",
    "    lgr_model = pickle.load(file)\n",
    "with open(\"xgb_model.pkl\", 'rb') as file:\n",
    "    xgb_model = pickle.load(file)\n",
    "with open(\"rf_model.pkl\", 'rb') as file:\n",
    "    rf_model = pickle.load(file)\n",
    "y_pred_lgr = lgr_model.predict(X_test)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "evaluate_model(y_test, y_pred_lgr, \"LogisticRegression\")\n",
    "evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "evaluate_model(y_test, y_pred_xgb, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_importances = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Random Forest Feature Importances: \")\n",
    "print(rf_importances)\n",
    "\n",
    "xgb_importances = pd.Series(best_xgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"XGBoost Feature Importances:\")\n",
    "print(xgb_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv_scores = cross_val_score(best_rf, X_scaled, y, cv=5)\n",
    "print(\"Random Forest Cross-Validation Accuracy:\", np.mean(rf_cv_scores))\n",
    "\n",
    "xgb_cv_scores = cross_val_score(best_xgb, X_scaled, y, cv=5)\n",
    "print(\"XGBoost Cross-Validation Accuracy:\", np.mean(xgb_cv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = ''\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
